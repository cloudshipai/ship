# LLM Integration in Ship CLI

## Current State

Ship CLI has a hybrid approach to AI-powered infrastructure investigation:

### 1. **Rule-Based System (Default)**
- Located in `internal/cli/ai_investigate_dynamic.go`
- Uses pattern matching and templates to convert natural language to Steampipe queries
- Provides instant responses without external API calls
- Works offline and doesn't require API keys

### 2. **LLM Module (Ready but not fully integrated)**
- Located in `internal/dagger/modules/llm.go`
- Supports OpenAI, Anthropic, and Ollama
- Currently uses curl-based API calls
- Can be activated with `--llm-provider` and `--model` flags

### 3. **Dagger LLM Support (Future)**
- Dagger is adding native LLM primitives: https://docs.dagger.io/features/llm/
- Would provide better integration and caching
- Currently not available in the Dagger Go SDK

## How AI Investigate Works Now

```bash
ship ai-investigate --prompt "Show me all public S3 buckets" --execute
```

1. **Query Generation**: Uses rule-based templates to convert natural language to SQL
2. **Execution**: Runs Steampipe queries in Dagger containers
3. **Analysis**: Uses pattern matching to identify security issues in results

## Enabling Full LLM Support

To use actual LLM for smarter queries and analysis:

```bash
# With OpenAI
export OPENAI_API_KEY=your-key
ship ai-investigate --prompt "Analyze my infrastructure for cost savings" \
  --llm-provider openai --model gpt-4 --execute

# With Anthropic  
export ANTHROPIC_API_KEY=your-key
ship ai-investigate --prompt "Find security vulnerabilities" \
  --llm-provider anthropic --model claude-3-opus --execute

# With local Ollama
ship ai-investigate --prompt "Check compliance issues" \
  --llm-provider ollama --model llama2 --execute
```

## Benefits of Current Approach

1. **No API Keys Required**: Works out of the box
2. **Fast**: No API latency
3. **Predictable**: Consistent query generation
4. **Free**: No API costs

## Benefits of LLM Integration

1. **Smarter Queries**: Better understanding of complex requests
2. **Deeper Analysis**: AI can spot patterns humans might miss  
3. **Custom Insights**: Tailored recommendations based on your infrastructure
4. **Learning**: Can improve over time with feedback

## Future Improvements

### 1. Native Dagger LLM Integration
Once Dagger's LLM primitives are available in Go SDK:
```go
result := client.LLM().
    WithModel("gpt-4").
    WithPrompt(prompt).
    WithSystemPrompt("You are a Steampipe expert...").
    Complete(ctx)
```

### 2. Hybrid Approach
- Use rule-based for common queries (fast, free)
- Escalate to LLM for complex analysis
- Cache LLM responses for similar queries

### 3. Fine-Tuned Models
- Train models specifically on Steampipe syntax
- Optimize for cloud infrastructure queries
- Reduce hallucinations in SQL generation

## Implementation Status

- ✅ Rule-based query generation
- ✅ LLM module structure
- ✅ Provider abstraction (OpenAI, Anthropic, Ollama)
- ⚠️  LLM integration in ai-investigate (partially implemented)
- ❌ Dagger native LLM support (waiting for SDK)
- ❌ Response caching
- ❌ Fine-tuned models

## Testing LLM Integration

```bash
# Test rule-based (default)
ship ai-investigate --prompt "Show EC2 instances" --execute

# Test with LLM (when API keys are configured)
ship ai-investigate --prompt "Show EC2 instances" --llm-provider openai --execute

# Compare the query plans generated by each approach
```

The system gracefully falls back to rule-based when:
- No LLM provider specified
- API keys missing
- LLM service unavailable
- Errors in LLM response